{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V-Sekai-fire/meshgpt-pytorch/blob/main/MeshGPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTLGOz_qqEpp"
      },
      "source": [
        "We use a local runtime. https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "soJKTsx9qE7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to /tmp/pip-req-build-nm9bfsnl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git /tmp/pip-req-build-nm9bfsnl\n",
            "  Resolved https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to commit 698c83562f8859c763880f200b210ff1081efedc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: beartype in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (0.17.2)\n",
            "Requirement already satisfied: einops>=0.7 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (0.7.0)\n",
            "Requirement already satisfied: ftfy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (2.24.0)\n",
            "Requirement already satisfied: torch>=2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (2.2.1)\n",
            "Requirement already satisfied: transformers[torch] in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (4.35.2)\n",
            "Requirement already satisfied: torchvision in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.17.1)\n",
            "Requirement already satisfied: regex in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (4.66.2)\n",
            "Requirement already satisfied: huggingface-hub in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.17.3)\n",
            "Requirement already satisfied: sentencepiece in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (4.25.3)\n",
            "Requirement already satisfied: timm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.9.16)\n",
            "Requirement already satisfied: filelock in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (4.10.0)\n",
            "Requirement already satisfied: sympy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (1.12)\n",
            "Requirement already satisfied: networkx in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.4.99)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from ftfy->classifier-free-guidance-pytorch==0.5.2) (0.2.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (6.0.1)\n",
            "Requirement already satisfied: requests in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (0.4.2)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (0.27.2)\n",
            "Requirement already satisfied: psutil in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.20.3->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (5.9.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from jinja2->torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from sympy->torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (9.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/meshgpt-pytorch.git to /tmp/pip-req-build-0_bnih0k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/meshgpt-pytorch.git /tmp/pip-req-build-0_bnih0k\n",
            "  Resolved https://github.com/MarcusLoppe/meshgpt-pytorch.git to commit d35228edc46b550dc8faeefb832db7bd43a23c2a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.25.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.27.2)\n",
            "Requirement already satisfied: beartype in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.17.2)\n",
            "Requirement already satisfied: classifier-free-guidance-pytorch>=0.5.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.5.2)\n",
            "Requirement already satisfied: einops>=0.7.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.7.0)\n",
            "Requirement already satisfied: einx>=0.1.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (0.1.3)\n",
            "Requirement already satisfied: ema-pytorch in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.4.2)\n",
            "Requirement already satisfied: local-attention>=1.9.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.9.0)\n",
            "Requirement already satisfied: gateloop-transformer>=0.2.2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.2.4)\n",
            "Requirement already satisfied: numpy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: pytorch-custom-utils>=0.0.9 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.0.18)\n",
            "Requirement already satisfied: taylor-series-linear-attention>=0.1.6 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.1.9)\n",
            "Requirement already satisfied: torch>=2.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (2.2.1)\n",
            "Requirement already satisfied: torch_geometric in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (2.5.0)\n",
            "Requirement already satisfied: torchtyping in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.1.4)\n",
            "Requirement already satisfied: tqdm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (4.66.2)\n",
            "Requirement already satisfied: vector-quantize-pytorch>=1.12.8 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.14.1)\n",
            "Requirement already satisfied: x-transformers>=1.26.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.27.19)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (23.2)\n",
            "Requirement already satisfied: psutil in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (0.17.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (0.4.2)\n",
            "Requirement already satisfied: ftfy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (2.24.0)\n",
            "Requirement already satisfied: transformers[torch] in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (4.35.2)\n",
            "Requirement already satisfied: sympy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from einx>=0.1.3->einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (1.12)\n",
            "Requirement already satisfied: frozendict in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from einx>=0.1.3->einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (2.4.0)\n",
            "Requirement already satisfied: rotary-embedding-torch in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from gateloop-transformer>=0.2.2->meshgpt-pytorch==1.0.2) (0.5.3)\n",
            "Requirement already satisfied: optree in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from pytorch-custom-utils>=0.0.9->meshgpt-pytorch==1.0.2) (0.10.0)\n",
            "Requirement already satisfied: pytorch-warmup in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from pytorch-custom-utils>=0.0.9->meshgpt-pytorch==1.0.2) (0.1.1)\n",
            "Requirement already satisfied: filelock in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (4.10.0)\n",
            "Requirement already satisfied: networkx in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->meshgpt-pytorch==1.0.2) (12.4.99)\n",
            "Requirement already satisfied: scipy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (1.12.0)\n",
            "Requirement already satisfied: aiohttp in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (3.9.3)\n",
            "Requirement already satisfied: requests in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (1.4.1.post1)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torchtyping->meshgpt-pytorch==1.0.2) (4.1.5)\n",
            "Requirement already satisfied: torchvision in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.17.1)\n",
            "Requirement already satisfied: regex in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (4.25.3)\n",
            "Requirement already satisfied: timm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.9.16)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (1.9.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from ftfy->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from jinja2->torch>=2.1->meshgpt-pytorch==1.0.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from scikit-learn->torch_geometric->meshgpt-pytorch==1.0.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from scikit-learn->torch_geometric->meshgpt-pytorch==1.0.2) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from sympy->einx>=0.1.3->einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.14.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (9.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: trimesh in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (4.1.7)\n",
            "Requirement already satisfied: numpy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from trimesh) (1.26.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!bash <(curl -L micro.mamba.pm/install.sh) \n",
        "#Prefix location? [~/micromamba] /workspace/micromamba\n",
        "#!pip3 install notebook jupyterlab\n",
        "#!micromamba create -n py311_meshgpt python=3.11 anaconda -c pytorch -c conda-forge -c anaconda -c defaults -y\n",
        "#!micromamba activate py311_meshgpt\n",
        "#!jupyter notebook --port=9999  --NotebookApp.port_retries=0 --allow-root\n",
        "# Login with kernel\n",
        "!pip3 install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!pip3 install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!pip3 install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "NM_rRocQAcZ_"
      },
      "outputs": [],
      "source": [
        "from re import T\n",
        "is_train_autoencoder = False\n",
        "is_train_autoencoder_disable_iteration = False\n",
        "is_train_mesh_transformer = True\n",
        "is_clear_dataset_npz = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5ztZ1JUl8zOZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import trimesh\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "max_faces = 4096\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    # Center and scale vertices\n",
        "    center = np.mean(mesh.vertices, axis=0)\n",
        "    vertices = mesh.vertices - center\n",
        "    max_abs = np.max(np.abs(vertices))\n",
        "    scale_factor = (1 / 128) / max_abs\n",
        "    vertices *= scale_factor\n",
        "\n",
        "    # Quantize vertices\n",
        "    vertices = np.around(vertices).astype(np.float32)\n",
        "\n",
        "    # Sort vertices by Z, Y, X\n",
        "    sorted_indices = np.lexsort(vertices.T[::-1])\n",
        "    vertices = vertices[sorted_indices]\n",
        "\n",
        "    # Map old indices to new, sorted indices\n",
        "    vertex_map = np.empty(len(sorted_indices), dtype=int)\n",
        "    vertex_map[sorted_indices] = np.arange(len(sorted_indices))\n",
        "\n",
        "    # Reindex faces\n",
        "    reindexed_faces = vertex_map[mesh.faces]\n",
        "    sorted_faces = np.sort(reindexed_faces, axis=1)\n",
        "\n",
        "    return vertices, sorted_faces\n",
        "\n",
        "def augment_mesh(vertices, jitter_strength=0.01):\n",
        "    jitter_amount = np.random.uniform(-jitter_strength, jitter_strength, size=vertices.shape)\n",
        "    vertices += jitter_amount\n",
        "    return vertices\n",
        "\n",
        "def snake_to_sentence_case(snake_str):\n",
        "    components = snake_str.split(\"_\")\n",
        "    return \" \".join(word.capitalize() for word in components)\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "\n",
        "    # Get random scale factors within a range\n",
        "    scale_factors = np.random.uniform(0.75, 1.0, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".glb\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            if len(faces) > max_faces:\n",
        "                print(f\"Mesh {filename} has {len(faces)} faces which is more than the allowed {max_faces} faces. Rejecting.\")\n",
        "                continue\n",
        "\n",
        "            faces_tensor = torch.tensor(faces, dtype=torch.long).to(\"cuda\")\n",
        "            face_edges = derive_face_edges_from_faces(faces_tensor)\n",
        "\n",
        "            text, _ = os.path.splitext(filename)\n",
        "            text = snake_to_sentence_case(text)\n",
        "            # Run video llava on the image. \"Describe the focus of the photo as a json dictionary.\"\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy()) * scale_factor\n",
        "                aug_vertices_tensor = torch.tensor(aug_vertices, dtype=torch.float)\n",
        "\n",
        "            obj_data = {\n",
        "                \"vertices\": aug_vertices_tensor.to(\"cuda\"),\n",
        "                \"faces\": faces_tensor,\n",
        "                \"face_edges\": face_edges,\n",
        "                \"texts\": text\n",
        "            }\n",
        "            obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_datas)} meshes\")\n",
        "    return obj_datas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "vthmcnU2dRmS",
        "outputId": "9cdca85c-3350-4c4d-869b-d82685259942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MeshDataset] Loaded 44 entrys\n",
            "[MeshDataset] Created from 44 entrys\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "project_name = \"meshgpt-pytorch\"\n",
        "\n",
        "working_dir = f'/workspace/{project_name}/dataset/blockmesh_test'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if is_clear_dataset_npz:\n",
        "    data = load_filename(working_dir, 1)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    dataset.save(dataset_path)\n",
        "else:\n",
        "    dataset = MeshDataset.load(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "vPpalWsbdRmU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0 complete. Processed texts Mire Clothing with 2179 vertices and 3884 faces.\n",
            "Iteration 1 complete. Processed texts S Bed Full with 62 vertices and 48 faces.\n",
            "Iteration 2 complete. Processed texts S Bed King with 403 vertices and 480 faces.\n",
            "Iteration 3 complete. Processed texts S Bed Twin with 403 vertices and 480 faces.\n",
            "Iteration 4 complete. Processed texts S Bone with 56 vertices and 24 faces.\n",
            "Iteration 5 complete. Processed texts S Box with 24 vertices and 12 faces.\n",
            "Iteration 6 complete. Processed texts S Cabinet Bookshelf with 690 vertices and 512 faces.\n",
            "Iteration 7 complete. Processed texts S Cabinet Dresser 03 with 552 vertices and 256 faces.\n",
            "Iteration 8 complete. Processed texts S Cabinet Dresser 05 with 744 vertices and 344 faces.\n",
            "Iteration 9 complete. Processed texts S Chair Bar with 385 vertices and 440 faces.\n",
            "Iteration 10 complete. Processed texts S Chair Box with 1848 vertices and 912 faces.\n",
            "Iteration 11 complete. Processed texts S Chair Modern with 904 vertices and 1176 faces.\n",
            "Iteration 12 complete. Processed texts S Chair Sofa with 1142 vertices and 1464 faces.\n",
            "Iteration 13 complete. Processed texts S Chair Sofa Wide with 702 vertices and 784 faces.\n",
            "Iteration 14 complete. Processed texts S Chair Stool with 848 vertices and 564 faces.\n",
            "Iteration 15 complete. Processed texts S Chair Stool Mini with 637 vertices and 564 faces.\n",
            "Iteration 16 complete. Processed texts S Door Double Frame with 38 vertices and 44 faces.\n",
            "Iteration 17 complete. Processed texts S Door Single with 1917 vertices and 2320 faces.\n",
            "Iteration 18 complete. Processed texts S Door Single Frame with 38 vertices and 44 faces.\n",
            "Iteration 19 complete. Processed texts S Gui with 40 vertices and 20 faces.\n",
            "Iteration 20 complete. Processed texts S Hmd with 451 vertices and 626 faces.\n",
            "Iteration 21 complete. Processed texts S Mask with 1089 vertices and 2090 faces.\n",
            "Iteration 22 complete. Processed texts S Phone with 1280 vertices and 1306 faces.\n",
            "Iteration 23 complete. Processed texts S Primitive Cylinder with 130 vertices and 128 faces.\n",
            "Iteration 24 complete. Processed texts S Primitive Cylinder Hollow with 128 vertices and 256 faces.\n",
            "Iteration 25 complete. Processed texts S Primitive Pyramid with 16 vertices and 6 faces.\n",
            "Iteration 26 complete. Processed texts S Primitive Sphere with 205 vertices and 320 faces.\n",
            "Iteration 27 complete. Processed texts S Primitive Wedge with 18 vertices and 8 faces.\n",
            "Iteration 28 complete. Processed texts S Stairs Single-6 with 96 vertices and 48 faces.\n",
            "Iteration 29 complete. Processed texts S Table Bar with 432 vertices and 216 faces.\n",
            "Iteration 30 complete. Processed texts S Table Bar Circle with 1486 vertices and 1376 faces.\n",
            "Iteration 31 complete. Processed texts S Table Bar Rectangle with 216 vertices and 108 faces.\n",
            "Iteration 32 complete. Processed texts S Table Bedside with 2388 vertices and 2184 faces.\n",
            "Iteration 33 complete. Processed texts S Table Counter with 96 vertices and 48 faces.\n",
            "Iteration 34 complete. Processed texts S Table Nightstand with 336 vertices and 168 faces.\n",
            "Iteration 35 complete. Processed texts S Table Office with 3112 vertices and 1556 faces.\n",
            "Iteration 36 complete. Processed texts S Table Sit Circle with 1486 vertices and 1376 faces.\n",
            "Iteration 37 complete. Processed texts S Table Sit Rectangle with 168 vertices and 144 faces.\n",
            "Iteration 38 complete. Processed texts S Table Sit Square with 168 vertices and 144 faces.\n",
            "Iteration 39 complete. Processed texts S Tree Bushy with 796 vertices and 910 faces.\n",
            "Iteration 40 complete. Processed texts S Tree No Leaves with 1197 vertices and 1486 faces.\n",
            "Iteration 41 complete. Processed texts S Ziggurat with 264 vertices and 335 faces.\n",
            "Iteration 42 complete. Processed texts Sk Cat 01 with 2074 vertices and 3638 faces.\n",
            "Iteration 43 complete. Processed texts Sk Snake 01 with 280 vertices and 556 faces.\n",
            "Total number of processed meshes: 44\n"
          ]
        }
      ],
      "source": [
        "seen_texts = []\n",
        "mesh_list = []  # List to store individual meshes\n",
        "translation_distance = 1.0  # Distance to translate vertices, set to 1 unit as required\n",
        "\n",
        "# Iterate over each item in the dataset\n",
        "for r, item in enumerate(dataset.data):\n",
        "    # Get vertices and faces\n",
        "    vertices = np.array(item['vertices'].cpu())\n",
        "    faces = np.array(item['faces'].cpu())\n",
        "    texts = item['texts']\n",
        "\n",
        "    if texts in seen_texts:\n",
        "      continue\n",
        "    seen_texts.append(texts)\n",
        "\n",
        "    # Translate the vertices copy\n",
        "    translation_vector = np.array([len(seen_texts) * translation_distance, 0, 0])  # Translation along the x-axis\n",
        "    vertices[:, :3] += translation_vector  # Apply translation only to x, y, z\n",
        "\n",
        "    # Create a new mesh object with the translated vertices and original faces\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "\n",
        "    # Append the new mesh to our list of meshes\n",
        "    mesh_list.append(mesh)\n",
        "\n",
        "    print(f\"Iteration {r} complete. Processed texts {texts} with {len(vertices)} vertices and {len(faces)} faces.\")\n",
        "\n",
        "# After iterating over all items, print the number of processed meshes\n",
        "print(f\"Total number of processed meshes: {len(mesh_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "JJ9qMRVxdRmW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(dataset.data)\n",
        "\n",
        "initial_length = len(dataset.data)\n",
        "target_length = 400\n",
        "print(initial_length)\n",
        "if initial_length > 0:\n",
        "    replication_factor = max(1, (target_length - 1) // initial_length + 1)\n",
        "    dataset.data *= replication_factor\n",
        "    dataset.data = dataset.data[:target_length]\n",
        "\n",
        "print(len(dataset.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "NKTuiTHvdRmY"
      },
      "outputs": [],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                            batch_size=8,\n",
        "                                            grad_accum_every=2,\n",
        "                                            learning_rate = 4e-3)\n",
        "if is_train_autoencoder:\n",
        "  if not is_train_autoencoder_disable_iteration:\n",
        "    autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  loss = autoencoder_trainer.train(380,stop_at_loss = 0.28, diplay_graph= True)\n",
        "  autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "else:\n",
        "  autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  autencoder = autoencoder_trainer.model\n",
        "  for param in autoencoder.parameters():\n",
        "      param.requires_grad = True\n",
        "  import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "gLCezPnNdRmZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest face count: 3884\n",
            "Max token sequence: 23304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder total parameters: 94.4M\n",
            "Total parameters: 152.6M\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "s--ya8W0dRmZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'S Chair Sofa', 'S Chair Bar', 'S Hmd', 'S Table Sit Square', 'S Chair Stool Mini', 'Mire Clothing', 'S Primitive Pyramid', 'S Chair Sofa Wide', 'S Bed Twin', 'S Door Single Frame', 'S Door Double Frame', 'S Cabinet Bookshelf', 'S Primitive Wedge', 'S Table Bar Rectangle', 'S Chair Stool', 'S Box', 'S Cabinet Dresser 05', 'S Primitive Cylinder', 'S Table Bar Circle', 'S Gui', 'S Bed King', 'S Table Nightstand', 'S Table Bedside', 'S Stairs Single-6', 'S Tree No Leaves', 'S Phone', 'S Bone', 'S Chair Modern', 'S Table Office', 'S Door Single', 'S Cabinet Dresser 03', 'S Bed Full', 'S Primitive Cylinder Hollow', 'S Primitive Sphere', 'Sk Snake 01', 'S Chair Box', 'S Table Counter', 'Sk Cat 01', 'S Table Bar', 'S Table Sit Circle', 'S Tree Bushy', 'S Mask', 'S Table Sit Rectangle', 'S Ziggurat'}\n",
            "[MeshDataset] Generated 44 text_embeddings\n",
            "[MeshDataset] Generated codes for 2000 entrys\n",
            "dict_keys(['vertices', 'faces', 'face_edges', 'text_embeds', 'codes'])\n"
          ]
        }
      ],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: Needs a fail early system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "I9hV_xUQdRma"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/7: 100%|| 2000/2000 [07:08<00:00,  4.66it/s, loss=7.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 average loss: 7.416166525118053\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/7: 100%|| 2000/2000 [07:09<00:00,  4.65it/s, loss=5.65]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 average loss: 4.328081157442182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/7: 100%|| 2000/2000 [07:07<00:00,  4.68it/s, loss=0.0952]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 average loss: 2.25699469096493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/7: 100%|| 2000/2000 [07:02<00:00,  4.73it/s, loss=0.0171]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 average loss: 0.9091332746301778          avg loss speed: 3.7579475165448777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/7: 100%|| 2000/2000 [07:05<00:00,  4.70it/s, loss=0.436]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 average loss: 0.26045236000115984          avg loss speed: 2.2376173476779364\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/7: 100%|| 2000/2000 [07:09<00:00,  4.65it/s, loss=0.0254]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 average loss: 0.054236595172202216          avg loss speed: 1.0879568466932203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/7: 100%|| 2000/2000 [06:59<00:00,  4.76it/s, loss=0.00704]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 average loss: 0.012523111718823202          avg loss speed: 0.3954176315490234\n",
            "Stopping training at epoch 6 with average loss 0.012523111718823202\n",
            "Training complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "steps = 100\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=steps, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=1)\n",
        "if is_train_mesh_transformer:\n",
        "  loss = trainer.train(steps, stop_at_loss = 0.02)\n",
        "else:\n",
        "  trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save(f'{working_dir}\\mesh-transformer_{project_name}.pt')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save(f'{working_dir}\\mesh-transformer_{project_name}.pt')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI5IM_Z3K26g"
      },
      "source": [
        "**Load the latest model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Q8by6SXp4GHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest face count: 3884\n",
            "Max token sequence: 23304\n"
          ]
        }
      ],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=1,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "autencoder = autoencoder_trainer.model\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = True\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    # attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ").to(\"cuda\")\n",
        "\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "trainer.load(f'{working_dir}\\mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating S Chair Sofa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|                                                                                                  | 10056/23304 [01:08<01:30, 146.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating S Chair Bar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 23304/23304 [02:39<00:00, 146.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating S Hmd\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 23304/23304 [02:38<00:00, 147.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating S Table Sit Square\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|                                                                                                                                                    | 2784/23304 [00:18<02:17, 149.66it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFaqEK_I8xDU"
      },
      "source": [
        "# **Preview 3d Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0xYQqQZ81nw"
      },
      "outputs": [],
      "source": [
        "!pip install ipyvolume trimesh pythreejs\n",
        "import trimesh\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "\n",
        "mesh = trimesh.load(f'{folder}/3d_models_all.obj')\n",
        "import ipyvolume as ipv\n",
        "\n",
        "vertices = mesh.vertices\n",
        "faces = mesh.faces\n",
        "\n",
        "fig = ipv.figure()\n",
        "ipv.plot_trisurf(vertices[:,0], vertices[:,1], vertices[:,2], triangles=faces)\n",
        "ipv.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens BROKEN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'texts'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[68], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdata:\n\u001b[0;32m----> 7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m label:\n\u001b[1;32m      8\u001b[0m             num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m token_length_procent)\n\u001b[1;32m     10\u001b[0m             texts\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[0;31mKeyError\u001b[0m: 'texts'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens BROKEN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'codes'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[74], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdata:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m label:\n\u001b[0;32m---> 26\u001b[0m         num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m token_length_procent)\n\u001b[1;32m     28\u001b[0m         texts\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m         codes\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()[:num_tokens]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
            "\u001b[0;31mKeyError\u001b[0m: 'codes'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_to_obj(vertices, faces, output_file_path):\n",
        "    scene = trimesh.Scene()\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        f.write(scene.export(file_type=\"obj\"))\n",
        "\n",
        "def encode_to_pua(codes):\n",
        "    flat_codes = [item for sublist in codes for subsublist in sublist for item in subsublist]\n",
        "    return \"\".join(chr(code + 0xF0000) for code in flat_codes)\n",
        "\n",
        "jsonl_lines = []\n",
        "\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, code in zip(texts, codes):\n",
        "        print(f\"Generating {text} with {code.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts=[text], prompt=code, temperature=0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        # Process mesh data inlined here\n",
        "        continuous_coors_list = [np_array.tolist() for np_array in faces_coordinates]\n",
        "        flat_list = [item for sublist in continuous_coors_list for item in sublist]\n",
        "        vertices = [vertex for sublist in flat_list for vertex in sublist]\n",
        "        faces = [[i, i + 1, i + 2] for i in range(0, len(vertices), 3)]\n",
        "\n",
        "        obj_filename = f'{text}_{code.shape[1]}_tokens.obj'\n",
        "        obj_file_path = folder / obj_filename\n",
        "        convert_to_obj(vertices, faces, obj_file_path)\n",
        "\n",
        "        encoded_codes = encode_to_pua(code.cpu().tolist())\n",
        "\n",
        "        with open(obj_file_path, \"r\") as file:\n",
        "            obj_contents = file.read()\n",
        "\n",
        "        # Append line to JSONL structure\n",
        "        jsonl_line = [\n",
        "            {\"role\": \"system\", \"content\": \"This assistant can understand 3D models using the meshgpt-pytorch Unicode plane 15 16384 item codebook for triangles and the .obj 3d format.\"},\n",
        "            {\"role\": \"user\", \"content\": encoded_codes},\n",
        "            {\"role\": \"assistant\", \"content\": obj_contents}\n",
        "        ]\n",
        "        jsonl_lines.append(jsonl_line)\n",
        "\n",
        "        print(obj_file_path)\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        "\n",
        "with open(\"chatml.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in jsonl_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
