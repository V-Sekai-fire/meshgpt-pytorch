{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V-Sekai-fire/meshgpt-pytorch/blob/main/MeshGPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTLGOz_qqEpp"
      },
      "source": [
        "We use a local runtime. https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "soJKTsx9qE7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to /tmp/pip-req-build-k11vcwtf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git /tmp/pip-req-build-k11vcwtf\n",
            "  Resolved https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to commit 698c83562f8859c763880f200b210ff1081efedc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: beartype in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (0.17.2)\n",
            "Requirement already satisfied: einops>=0.7 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (0.7.0)\n",
            "Requirement already satisfied: ftfy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (2.24.0)\n",
            "Requirement already satisfied: torch>=2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (2.2.1)\n",
            "Requirement already satisfied: transformers[torch] in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch==0.5.2) (4.35.2)\n",
            "Requirement already satisfied: torchvision in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.17.1)\n",
            "Requirement already satisfied: regex in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (4.66.2)\n",
            "Requirement already satisfied: huggingface-hub in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.17.3)\n",
            "Requirement already satisfied: sentencepiece in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (4.25.3)\n",
            "Requirement already satisfied: timm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (0.9.16)\n",
            "Requirement already satisfied: filelock in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (4.10.0)\n",
            "Requirement already satisfied: sympy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (1.12)\n",
            "Requirement already satisfied: networkx in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (12.4.99)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from ftfy->classifier-free-guidance-pytorch==0.5.2) (0.2.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (6.0.1)\n",
            "Requirement already satisfied: requests in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (0.4.2)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (0.27.2)\n",
            "Requirement already satisfied: psutil in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.20.3->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (5.9.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from jinja2->torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from sympy->torch>=2.0->classifier-free-guidance-pytorch==0.5.2) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.2) (9.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/meshgpt-pytorch.git to /tmp/pip-req-build-pxyk2f_w\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/meshgpt-pytorch.git /tmp/pip-req-build-pxyk2f_w\n",
            "  Resolved https://github.com/MarcusLoppe/meshgpt-pytorch.git to commit d35228edc46b550dc8faeefb832db7bd43a23c2a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.25.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.27.2)\n",
            "Requirement already satisfied: beartype in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.17.2)\n",
            "Requirement already satisfied: classifier-free-guidance-pytorch>=0.5.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.5.2)\n",
            "Requirement already satisfied: einops>=0.7.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.7.0)\n",
            "Requirement already satisfied: einx>=0.1.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (0.1.3)\n",
            "Requirement already satisfied: ema-pytorch in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.4.2)\n",
            "Requirement already satisfied: local-attention>=1.9.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.9.0)\n",
            "Requirement already satisfied: gateloop-transformer>=0.2.2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.2.4)\n",
            "Requirement already satisfied: numpy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: pytorch-custom-utils>=0.0.9 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.0.18)\n",
            "Requirement already satisfied: taylor-series-linear-attention>=0.1.6 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.1.9)\n",
            "Requirement already satisfied: torch>=2.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (2.2.1)\n",
            "Requirement already satisfied: torch_geometric in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (2.5.0)\n",
            "Requirement already satisfied: torchtyping in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (0.1.4)\n",
            "Requirement already satisfied: tqdm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (4.66.2)\n",
            "Requirement already satisfied: vector-quantize-pytorch>=1.12.8 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.14.1)\n",
            "Requirement already satisfied: x-transformers>=1.26.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from meshgpt-pytorch==1.0.2) (1.27.19)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (23.2)\n",
            "Requirement already satisfied: psutil in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (0.17.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.0.2) (0.4.2)\n",
            "Requirement already satisfied: ftfy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (2.24.0)\n",
            "Requirement already satisfied: transformers[torch] in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (4.35.2)\n",
            "Requirement already satisfied: sympy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from einx>=0.1.3->einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (1.12)\n",
            "Requirement already satisfied: frozendict in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from einx>=0.1.3->einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (2.4.0)\n",
            "Requirement already satisfied: rotary-embedding-torch in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from gateloop-transformer>=0.2.2->meshgpt-pytorch==1.0.2) (0.5.3)\n",
            "Requirement already satisfied: optree in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from pytorch-custom-utils>=0.0.9->meshgpt-pytorch==1.0.2) (0.10.0)\n",
            "Requirement already satisfied: pytorch-warmup in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from pytorch-custom-utils>=0.0.9->meshgpt-pytorch==1.0.2) (0.1.1)\n",
            "Requirement already satisfied: filelock in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (4.10.0)\n",
            "Requirement already satisfied: networkx in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch>=2.1->meshgpt-pytorch==1.0.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->meshgpt-pytorch==1.0.2) (12.4.99)\n",
            "Requirement already satisfied: scipy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (1.12.0)\n",
            "Requirement already satisfied: aiohttp in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (3.9.3)\n",
            "Requirement already satisfied: requests in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torch_geometric->meshgpt-pytorch==1.0.2) (1.4.1.post1)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torchtyping->meshgpt-pytorch==1.0.2) (4.1.5)\n",
            "Requirement already satisfied: torchvision in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.17.1)\n",
            "Requirement already satisfied: regex in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (4.25.3)\n",
            "Requirement already satisfied: timm in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.9.16)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.0.2) (1.9.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from ftfy->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from jinja2->torch>=2.1->meshgpt-pytorch==1.0.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from requests->torch_geometric->meshgpt-pytorch==1.0.2) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from scikit-learn->torch_geometric->meshgpt-pytorch==1.0.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from scikit-learn->torch_geometric->meshgpt-pytorch==1.0.2) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from sympy->einx>=0.1.3->einx[torch]>=0.1.3->meshgpt-pytorch==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from transformers[torch]->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (0.14.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.5.1->meshgpt-pytorch==1.0.2) (9.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: trimesh in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (4.1.7)\n",
            "Requirement already satisfied: numpy in /root/micromamba/envs/py311_meshgpt/lib/python3.11/site-packages (from trimesh) (1.26.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!bash <(curl -L micro.mamba.pm/install.sh) \n",
        "#Prefix location? [~/micromamba] /workspace/micromamba\n",
        "#!pip3 install notebook jupyterlab\n",
        "#!micromamba create -n py311_meshgpt python=3.11 anaconda -c pytorch -c conda-forge -c anaconda -c defaults -y\n",
        "#!micromamba activate py311_meshgpt\n",
        "#!jupyter notebook --port=9999  --NotebookApp.port_retries=0 --allow-root\n",
        "# Login with kernel\n",
        "!pip3 install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!pip3 install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!pip3 install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NM_rRocQAcZ_"
      },
      "outputs": [],
      "source": [
        "from re import T\n",
        "is_train_autoencoder = True\n",
        "is_train_autoencoder_disable_iteration = True\n",
        "is_train_mesh_transformer = True\n",
        "is_clear_dataset_npz = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5ztZ1JUl8zOZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import trimesh\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "max_faces = 4096\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    # Center and scale vertices\n",
        "    center = np.mean(mesh.vertices, axis=0)\n",
        "    vertices = mesh.vertices - center\n",
        "    max_abs = np.max(np.abs(vertices))\n",
        "    scale_factor = (1 / 128) / max_abs\n",
        "    vertices *= scale_factor\n",
        "\n",
        "    # Quantize vertices\n",
        "    vertices = np.around(vertices).astype(np.float32)\n",
        "\n",
        "    # Sort vertices by Z, Y, X\n",
        "    sorted_indices = np.lexsort(vertices.T[::-1])\n",
        "    vertices = vertices[sorted_indices]\n",
        "\n",
        "    # Map old indices to new, sorted indices\n",
        "    vertex_map = np.empty(len(sorted_indices), dtype=int)\n",
        "    vertex_map[sorted_indices] = np.arange(len(sorted_indices))\n",
        "\n",
        "    # Reindex faces\n",
        "    reindexed_faces = vertex_map[mesh.faces]\n",
        "    sorted_faces = np.sort(reindexed_faces, axis=1)\n",
        "\n",
        "    return vertices, sorted_faces\n",
        "\n",
        "def augment_mesh(vertices, jitter_strength=0.01):\n",
        "    jitter_amount = np.random.uniform(-jitter_strength, jitter_strength, size=vertices.shape)\n",
        "    vertices += jitter_amount\n",
        "    return vertices\n",
        "\n",
        "def snake_to_sentence_case(snake_str):\n",
        "    components = snake_str.split(\"_\")\n",
        "    return \" \".join(word.capitalize() for word in components)\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "\n",
        "    # Get random scale factors within a range\n",
        "    scale_factors = np.random.uniform(0.75, 1.0, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".glb\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            if len(faces) > max_faces:\n",
        "                print(f\"Mesh {filename} has {len(faces)} faces which is more than the allowed {max_faces} faces. Rejecting.\")\n",
        "                continue\n",
        "\n",
        "            faces_tensor = torch.tensor(faces, dtype=torch.long).to(\"cuda\")\n",
        "            face_edges = derive_face_edges_from_faces(faces_tensor)\n",
        "\n",
        "            text, _ = os.path.splitext(filename)\n",
        "            text = snake_to_sentence_case(text)\n",
        "            # Run video llava on the image. \"Describe the focus of the photo as a json dictionary.\"\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy()) * scale_factor\n",
        "                aug_vertices_tensor = torch.tensor(aug_vertices, dtype=torch.float)\n",
        "\n",
        "            obj_data = {\n",
        "                \"vertices\": aug_vertices_tensor.to(\"cuda\"),\n",
        "                \"faces\": faces_tensor,\n",
        "                \"face_edges\": face_edges,\n",
        "                \"texts\": text\n",
        "            }\n",
        "            obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_datas)} meshes\")\n",
        "    return obj_datas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "vthmcnU2dRmS",
        "outputId": "9cdca85c-3350-4c4d-869b-d82685259942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mesh s_chair_office.glb has 6894 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_chair_office_01.glb has 25192 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_desk.glb has 7240 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_door_double.glb has 4424 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_form_torso_dress_feminine.glb has 21754 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_sgcontroller.glb has 8848 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_skate_park.glb has 8598 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_table_coffee.glb has 4304 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_table_coffee_variation.glb has 8140 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh s_vehicle_bicycle_bmx.glb has 39972 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_baby_01.glb has 13408 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_child_01.glb has 13408 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_dog_01.glb has 8418 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_feminine_adult.glb has 13358 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_feminine_teen.glb has 13358 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_horse_01.glb has 4447 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_masculine_adult.glb has 13410 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_masculine_adult_ik.glb has 13410 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "Mesh sk_masculine_teen.glb has 13408 faces which is more than the allowed 4096 faces. Rejecting.\n",
            "[create_mesh_dataset] Returning 44 meshes\n",
            "[MeshDataset] Created from 44 entrys\n",
            "[MeshDataset] Generated face_edges for 0/44 entrys\n",
            "[MeshDataset] Saved 44 entrys at /workspace/meshgpt-pytorch/dataset/blockmesh_test/meshgpt-pytorch.npz\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "working_dir = f'/workspace/meshgpt-pytorch/dataset/blockmesh_test'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if is_clear_dataset_npz:\n",
        "    data = load_filename(working_dir, 1)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    dataset.save(dataset_path)\n",
        "else:\n",
        "    dataset = MeshDataset.load(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vPpalWsbdRmU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0 complete. Processed texts Mire Clothing with 2179 vertices and 3884 faces.\n",
            "Iteration 1 complete. Processed texts S Bed Full with 62 vertices and 48 faces.\n",
            "Iteration 2 complete. Processed texts S Bed King with 403 vertices and 480 faces.\n",
            "Iteration 3 complete. Processed texts S Bed Twin with 403 vertices and 480 faces.\n",
            "Iteration 4 complete. Processed texts S Bone with 56 vertices and 24 faces.\n",
            "Iteration 5 complete. Processed texts S Box with 24 vertices and 12 faces.\n",
            "Iteration 6 complete. Processed texts S Cabinet Bookshelf with 690 vertices and 512 faces.\n",
            "Iteration 7 complete. Processed texts S Cabinet Dresser 03 with 552 vertices and 256 faces.\n",
            "Iteration 8 complete. Processed texts S Cabinet Dresser 05 with 744 vertices and 344 faces.\n",
            "Iteration 9 complete. Processed texts S Chair Bar with 385 vertices and 440 faces.\n",
            "Iteration 10 complete. Processed texts S Chair Box with 1848 vertices and 912 faces.\n",
            "Iteration 11 complete. Processed texts S Chair Modern with 904 vertices and 1176 faces.\n",
            "Iteration 12 complete. Processed texts S Chair Sofa with 1142 vertices and 1464 faces.\n",
            "Iteration 13 complete. Processed texts S Chair Sofa Wide with 702 vertices and 784 faces.\n",
            "Iteration 14 complete. Processed texts S Chair Stool with 848 vertices and 564 faces.\n",
            "Iteration 15 complete. Processed texts S Chair Stool Mini with 637 vertices and 564 faces.\n",
            "Iteration 16 complete. Processed texts S Door Double Frame with 38 vertices and 44 faces.\n",
            "Iteration 17 complete. Processed texts S Door Single with 1917 vertices and 2320 faces.\n",
            "Iteration 18 complete. Processed texts S Door Single Frame with 38 vertices and 44 faces.\n",
            "Iteration 19 complete. Processed texts S Gui with 40 vertices and 20 faces.\n",
            "Iteration 20 complete. Processed texts S Hmd with 451 vertices and 626 faces.\n",
            "Iteration 21 complete. Processed texts S Mask with 1089 vertices and 2090 faces.\n",
            "Iteration 22 complete. Processed texts S Phone with 1280 vertices and 1306 faces.\n",
            "Iteration 23 complete. Processed texts S Primitive Cylinder with 130 vertices and 128 faces.\n",
            "Iteration 24 complete. Processed texts S Primitive Cylinder Hollow with 128 vertices and 256 faces.\n",
            "Iteration 25 complete. Processed texts S Primitive Pyramid with 16 vertices and 6 faces.\n",
            "Iteration 26 complete. Processed texts S Primitive Sphere with 205 vertices and 320 faces.\n",
            "Iteration 27 complete. Processed texts S Primitive Wedge with 18 vertices and 8 faces.\n",
            "Iteration 28 complete. Processed texts S Stairs Single-6 with 96 vertices and 48 faces.\n",
            "Iteration 29 complete. Processed texts S Table Bar with 432 vertices and 216 faces.\n",
            "Iteration 30 complete. Processed texts S Table Bar Circle with 1486 vertices and 1376 faces.\n",
            "Iteration 31 complete. Processed texts S Table Bar Rectangle with 216 vertices and 108 faces.\n",
            "Iteration 32 complete. Processed texts S Table Bedside with 2388 vertices and 2184 faces.\n",
            "Iteration 33 complete. Processed texts S Table Counter with 96 vertices and 48 faces.\n",
            "Iteration 34 complete. Processed texts S Table Nightstand with 336 vertices and 168 faces.\n",
            "Iteration 35 complete. Processed texts S Table Office with 3112 vertices and 1556 faces.\n",
            "Iteration 36 complete. Processed texts S Table Sit Circle with 1486 vertices and 1376 faces.\n",
            "Iteration 37 complete. Processed texts S Table Sit Rectangle with 168 vertices and 144 faces.\n",
            "Iteration 38 complete. Processed texts S Table Sit Square with 168 vertices and 144 faces.\n",
            "Iteration 39 complete. Processed texts S Tree Bushy with 796 vertices and 910 faces.\n",
            "Iteration 40 complete. Processed texts S Tree No Leaves with 1197 vertices and 1486 faces.\n",
            "Iteration 41 complete. Processed texts S Ziggurat with 264 vertices and 335 faces.\n",
            "Iteration 42 complete. Processed texts Sk Cat 01 with 2074 vertices and 3638 faces.\n",
            "Iteration 43 complete. Processed texts Sk Snake 01 with 280 vertices and 556 faces.\n",
            "Total number of processed meshes: 44\n"
          ]
        }
      ],
      "source": [
        "seen_texts = []\n",
        "mesh_list = []  # List to store individual meshes\n",
        "translation_distance = 1.0  # Distance to translate vertices, set to 1 unit as required\n",
        "\n",
        "# Iterate over each item in the dataset\n",
        "for r, item in enumerate(dataset.data):\n",
        "    # Get vertices and faces\n",
        "    vertices = np.array(item['vertices'].cpu())\n",
        "    faces = np.array(item['faces'].cpu())\n",
        "    texts = item['texts']\n",
        "\n",
        "    if texts in seen_texts:\n",
        "      continue\n",
        "    seen_texts.append(texts)\n",
        "\n",
        "    # Translate the vertices copy\n",
        "    translation_vector = np.array([len(seen_texts) * translation_distance, 0, 0])  # Translation along the x-axis\n",
        "    vertices[:, :3] += translation_vector  # Apply translation only to x, y, z\n",
        "\n",
        "    # Create a new mesh object with the translated vertices and original faces\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "\n",
        "    # Append the new mesh to our list of meshes\n",
        "    mesh_list.append(mesh)\n",
        "\n",
        "    print(f\"Iteration {r} complete. Processed texts {texts} with {len(vertices)} vertices and {len(faces)} faces.\")\n",
        "\n",
        "# After iterating over all items, print the number of processed meshes\n",
        "print(f\"Total number of processed meshes: {len(mesh_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JJ9qMRVxdRmW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(dataset.data)\n",
        "\n",
        "initial_length = len(dataset.data)\n",
        "target_length = 2000\n",
        "print(initial_length)\n",
        "if initial_length > 0:\n",
        "    replication_factor = max(1, (target_length - 1) // initial_length + 1)\n",
        "    dataset.data *= replication_factor\n",
        "    dataset.data = dataset.data[:target_length]\n",
        "\n",
        "print(len(dataset.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NKTuiTHvdRmY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/380:   0%|                                                                                                                                                     | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/380: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:51<00:00,  4.83it/s, commit_loss=0.538, loss=0.921, recon_loss=0.867]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 average loss: 1.0335369660854339 recon loss: 0.9772: commit_loss 0.5633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/380: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:47<00:00,  5.23it/s, commit_loss=0.543, loss=0.763, recon_loss=0.709]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 average loss: 0.8711508121490479 recon loss: 0.8294: commit_loss 0.4174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/380:  46%|████████████████████████████████████████▌                                                | 114/250 [00:22<00:26,  5.06it/s, commit_loss=0.0224, loss=0.689, recon_loss=0.686]"
          ]
        }
      ],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                            batch_size=8,\n",
        "                                            grad_accum_every=2,\n",
        "                                            learning_rate = 4e-3)\n",
        "if is_train_autoencoder:\n",
        "  if not is_train_autoencoder_disable_iteration:\n",
        "    autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  loss = autoencoder_trainer.train(380,stop_at_loss = 0.28, diplay_graph= True)\n",
        "  autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "else:\n",
        "  autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  autencoder = autoencoder_trainer.model\n",
        "  for param in autoencoder.parameters():\n",
        "      param.requires_grad = True\n",
        "  import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLCezPnNdRmZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s--ya8W0dRmZ"
      },
      "outputs": [],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9hV_xUQdRma"
      },
      "outputs": [],
      "source": [
        "if is_train_mesh_transformer:\n",
        "  trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=1)\n",
        "  loss = trainer.train(200, stop_at_loss = 0.00001)\n",
        "else:\n",
        "  trainer = MeshTransformerTrainer(model = transformer, warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=1)\n",
        "  trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "  transformer = trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI5IM_Z3K26g"
      },
      "source": [
        "**Load the latest model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8by6SXp4GHd"
      },
      "outputs": [],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=1,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "autencoder = autoencoder_trainer.model\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = True\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    # attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ").to(\"cuda\")\n",
        "\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlzLzYxdRma"
      },
      "outputs": [],
      "source": [
        "trainer.save(f'{working_dir}/mesh-transformer_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFaqEK_I8xDU"
      },
      "source": [
        "# **Preview 3d Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0xYQqQZ81nw"
      },
      "outputs": [],
      "source": [
        "!pip install ipyvolume trimesh pythreejs\n",
        "import trimesh\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "\n",
        "mesh = trimesh.load(f'{folder}/3d_models_all.obj')\n",
        "import ipyvolume as ipv\n",
        "\n",
        "vertices = mesh.vertices\n",
        "faces = mesh.faces\n",
        "\n",
        "fig = ipv.figure()\n",
        "ipv.plot_trisurf(vertices[:,0], vertices[:,1], vertices[:,2], triangles=faces)\n",
        "ipv.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_to_obj(vertices, faces, output_file_path):\n",
        "    scene = trimesh.Scene()\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        f.write(scene.export(file_type=\"obj\"))\n",
        "\n",
        "def encode_to_pua(codes):\n",
        "    flat_codes = [item for sublist in codes for subsublist in sublist for item in subsublist]\n",
        "    return \"\".join(chr(code + 0xF0000) for code in flat_codes)\n",
        "\n",
        "jsonl_lines = []\n",
        "\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, code in zip(texts, codes):\n",
        "        print(f\"Generating {text} with {code.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts=[text], prompt=code, temperature=0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        # Process mesh data inlined here\n",
        "        continuous_coors_list = [np_array.tolist() for np_array in faces_coordinates]\n",
        "        flat_list = [item for sublist in continuous_coors_list for item in sublist]\n",
        "        vertices = [vertex for sublist in flat_list for vertex in sublist]\n",
        "        faces = [[i, i + 1, i + 2] for i in range(0, len(vertices), 3)]\n",
        "\n",
        "        obj_filename = f'{text}_{code.shape[1]}_tokens.obj'\n",
        "        obj_file_path = folder / obj_filename\n",
        "        convert_to_obj(vertices, faces, obj_file_path)\n",
        "\n",
        "        encoded_codes = encode_to_pua(code.cpu().tolist())\n",
        "\n",
        "        with open(obj_file_path, \"r\") as file:\n",
        "            obj_contents = file.read()\n",
        "\n",
        "        # Append line to JSONL structure\n",
        "        jsonl_line = [\n",
        "            {\"role\": \"system\", \"content\": \"This assistant can understand 3D models using the meshgpt-pytorch Unicode plane 15 16384 item codebook for triangles and the .obj 3d format.\"},\n",
        "            {\"role\": \"user\", \"content\": encoded_codes},\n",
        "            {\"role\": \"assistant\", \"content\": obj_contents}\n",
        "        ]\n",
        "        jsonl_lines.append(jsonl_line)\n",
        "\n",
        "        print(obj_file_path)\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        "\n",
        "with open(\"chatml.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in jsonl_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
