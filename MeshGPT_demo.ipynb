{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "!pip install  git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
            "!pip install  -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import trimesh\n",
            "import numpy as np\n",
            "import os\n",
            "import csv\n",
            "from collections import OrderedDict\n",
            "\n",
            "from meshgpt_pytorch import (\n",
            "    MeshTransformerTrainer,\n",
            "    MeshAutoencoderTrainer,\n",
            "    MeshAutoencoder,\n",
            "    MeshTransformer\n",
            ")\n",
            "from meshgpt_pytorch.data import ( \n",
            "    derive_face_edges_from_faces\n",
            ") \n",
            "\n",
            "def get_mesh(file_path): \n",
            "    mesh = trimesh.load(file_path, force='mesh')\n",
            "     \n",
            "    vertices = mesh.vertices.tolist()\n",
            "    faces = mesh.faces.tolist()\n",
            "    # Center\n",
            "    centered_vertices = vertices - np.mean(vertices, axis=0)\n",
            "    # Limit vertices to [-0.95, 0.95]\n",
            "    max_abs = np.max(np.abs(centered_vertices))\n",
            "    vertices = centered_vertices / (max_abs / 0.95)  \n",
            "    \n",
            "    # Sort by Z, Y,X where Z is vertical    \n",
            "    def sort_vertices(vertex):\n",
            "        return vertex[2], vertex[1], vertex[0]   \n",
            " \n",
            "    seen = OrderedDict()\n",
            "    for point in vertices: \n",
            "      key = tuple(point)\n",
            "      if key not in seen:\n",
            "        seen[key] = point\n",
            "        \n",
            "    unique_vertices =  list(seen.values()) \n",
            "    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n",
            "      \n",
            "    vertices_as_tuples = [tuple(v) for v in vertices]\n",
            "    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n",
            "\n",
            "    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple} \n",
            "    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces]   \n",
            "    sorted_faces = [sorted(sub_arr) for sub_arr in reindexed_faces]  \n",
            "    \n",
            "    return np.array(sorted_vertices), np.array(sorted_faces)\n",
            " \n",
            " \n",
            "\n",
            "def augment_mesh(vertices, scale_factor):     \n",
            "    jitter_factor=0.01 \n",
            "    possible_values = np.arange(-jitter_factor, jitter_factor , 0.0005) \n",
            "    offsets = np.random.choice(possible_values, size=vertices.shape) \n",
            "    vertices = vertices + offsets   \n",
            "    \n",
            "    vertices = vertices * scale_factor \n",
            "    return vertices\n",
            "\n",
            "   \n",
            "def load_filename(directory, variations):\n",
            "    obj_datas = []   \n",
            "    chosen_models_count = {}      \n",
            "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for filename in os.listdir(directory):\n",
            "        if filename.endswith((\".obj\", \".glb\", \".off\")): \n",
            "            file_path = os.path.join(directory, filename) \n",
            "            vertices, faces = get_mesh(file_path)  \n",
            "            \n",
            "            faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"cuda\")\n",
            "            face_edges =  derive_face_edges_from_faces(faces)  \n",
            "            texts, ext = os.path.splitext(filename)     \n",
            "            \n",
            "            for scale_factor in scale_factors: \n",
            "                aug_vertices = augment_mesh(vertices.copy(), scale_factor)  \n",
            "                obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"cuda\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": texts } \n",
            "                obj_datas.append(obj_data)\n",
            "                     \n",
            "    print(f\"[create_mesh_dataset] Returning {len(obj_data)} meshes\")\n",
            "    return obj_datas"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path \n",
            "import gc    \n",
            "import torch\n",
            "import os\n",
            "from meshgpt_pytorch import MeshDataset\n",
            "\n",
            "torch.cuda.empty_cache()\n",
            "gc.collect()  \n",
            "\n",
            " \n",
            "project_name = \"demo_mesh\" \n",
            "\n",
            "working_dir = f'.\\{project_name}'\n",
            "\n",
            "working_dir = Path(working_dir)\n",
            "working_dir.mkdir(exist_ok = True, parents = True)\n",
            "dataset_path = working_dir / (project_name + \".npz\")\n",
            "\n",
            "\n",
            "if not os.path.isfile(dataset_path):\n",
            "    data = load_filename(\"./demo_mesh\",10)  \n",
            "    dataset = MeshDataset(data) \n",
            "    dataset.generate_face_edges() \n",
            "    print(set(item[\"texts\"] for item in dataset.data)  ) \n",
            "    dataset.save(dataset_path)\n",
            " \n",
            "dataset = MeshDataset.load(dataset_path) \n",
            "print(dataset.data[0].keys())\n",
            " "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Inspect"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            " \n",
            "folder = working_dir / f'renders' \n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
            "   \n",
            "all_vertices = []\n",
            "all_faces = []\n",
            "vertex_offset = 0\n",
            "translation_distance = 0.5  \n",
            "\n",
            "for r, item in enumerate(data): \n",
            "    vertices_copy =  np.copy(item['vertices'])\n",
            "    vertices_copy += translation_distance * (r / 0.2 - 1) \n",
            "    \n",
            "    for vert in vertices_copy:\n",
            "        vertex = vert.to('cpu')\n",
            "        all_vertices.append(f\"v {float(vertex[0])}  {float(vertex[1])}  {float(vertex[2])}\\n\") \n",
            "    for face in item['faces']:\n",
            "        all_faces.append(f\"f {face[0]+1+ vertex_offset} {face[1]+ 1+vertex_offset} {face[2]+ 1+vertex_offset}\\n\")  \n",
            "    vertex_offset = len(all_vertices)\n",
            " \n",
            "obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
            " \n",
            "obj_file_path = f'{folder}/3d_models_inspect.obj'\n",
            "\n",
            "with open(obj_file_path, \"w\") as file:\n",
            "    file.write(obj_file_content)    \n",
            "    "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Train!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "autoencoder = MeshAutoencoder().to(\"cuda\")      "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset.data = [dict(d) for d in dataset.data] * 10\n",
            "print(len(dataset.data))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "*Load previous saved model if you had to restart session*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4) \n",
            "autoencoder_trainer.load(f'{working_dir}\\mesh-encoder_{project_name}.pt')   \n",
            "autencoder = autoencoder_trainer.model\n",
            "for param in autoencoder.parameters():\n",
            "    param.requires_grad = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Train to about 0.3 loss if you are using a small dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
            "                                             batch_size=8,\n",
            "                                             grad_accum_every=2,\n",
            "                                             learning_rate = 1e-2) \n",
            "loss = autoencoder_trainer.train(280,stop_at_loss = 0.7, diplay_graph= True)   \n",
            "  \n",
            "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
            "                                             batch_size=8,\n",
            "                                             grad_accum_every=2,\n",
            "                                             learning_rate = 4e-3) \n",
            "loss = autoencoder_trainer.train(280,stop_at_loss = 0.28, diplay_graph= True)     "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "autoencoder_trainer.save(f'{working_dir}\\mesh-encoder_{project_name}.pt')   "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import gc  \n",
            "torch.cuda.empty_cache()\n",
            "gc.collect()  \n",
            "\n",
            "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d) \n",
            "max_seq = max_length * 6  \n",
            "print(\"Highest face count:\" , max_length)\n",
            "print(\"Max token sequence:\" , max_seq) \n",
            "\n",
            "transformer = MeshTransformer(\n",
            "    autoencoder,\n",
            "    dim = 512,\n",
            "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
            "    fine_pre_gateloop_depth= 4, \n",
            "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
            "    max_seq_len = max_seq, \n",
            "    condition_on_text = True, \n",
            "    gateloop_use_heinsen = False,\n",
            "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
            "    text_condition_cond_drop_prob = 0.0\n",
            ") \n",
            "\n",
            "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
            "total_params = f\"{total_params / 1000000:.1f}M\"\n",
            "print(f\"Decoder total parameters: {total_params}\") \n",
            "total_params = sum(p.numel() for p in transformer.parameters())\n",
            "total_params = f\"{total_params / 1000000:.1f}M\"\n",
            "print(f\"Total parameters: {total_params}\") "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
            "\n",
            "**If you don't;** <br>\n",
            "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
            "<br>\n",
            "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
            "\n",
            "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labels = set(item[\"texts\"] for item in dataset.data)\n",
            "print(labels)\n",
            "dataset.embed_texts(transformer)\n",
            "dataset.generate_codes(autoencoder)\n",
            "print(dataset.data[0].keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "*Load previous saved model if you had to restart session*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
            "trainer.load(f'{working_dir}\\mesh-transformer_{project_name}.pt')  \n",
            "transformer = trainer.model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
            "                                 learning_rate = 1e-3, batch_size=2)  \n",
            "loss = trainer.train(100, stop_at_loss = 0.009) \n",
            "\n",
            "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
            "                                 learning_rate = 5e-4, batch_size=2)\n",
            "loss = trainer.train(200, stop_at_loss = 0.00001)  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            " \n",
            "trainer.save(f'{working_dir}\\mesh-transformer_{project_name}.pt')   "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Generate and view mesh"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def combind_mesh(path, mesh):\n",
            "    all_vertices = []\n",
            "    all_faces = []\n",
            "    vertex_offset = 0\n",
            "    translation_distance = 0.5  \n",
            "\n",
            "    for r, faces_coordinates in enumerate(mesh): \n",
            "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)   \n",
            "    \n",
            "        for vertex in numpy_data:\n",
            "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
            "    \n",
            "        for i in range(1, len(numpy_data), 3):\n",
            "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
            "    \n",
            "        vertex_offset += len(numpy_data)\n",
            "    \n",
            "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
            "     \n",
            "    with open(path , \"w\") as file:\n",
            "        file.write(obj_file_content)   \n",
            " \n",
            "def combind_mesh_with_rows(path, meshes):\n",
            "    all_vertices = []\n",
            "    all_faces = []\n",
            "    vertex_offset = 0\n",
            "    translation_distance = 0.5  \n",
            "    \n",
            "    for row, mesh in enumerate(meshes): \n",
            "        for r, faces_coordinates in enumerate(mesh): \n",
            "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)  \n",
            "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)  \n",
            "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)  \n",
            "        \n",
            "            for vertex in numpy_data:\n",
            "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
            "        \n",
            "            for i in range(1, len(numpy_data), 3):\n",
            "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
            "        \n",
            "            vertex_offset += len(numpy_data)\n",
            "        \n",
            "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
            "     \n",
            "    with open(path , \"w\") as file:\n",
            "        file.write(obj_file_content)   \n",
            "        \n",
            "        \n",
            "def write_mesh_output(path, coords):\n",
            "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)  \n",
            "    obj_file_content = \"\"\n",
            "    \n",
            "    for vertex in numpy_data:\n",
            "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
            "\n",
            "    for i in range(1, len(numpy_data), 3):\n",
            "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
            " \n",
            "    with open(path, \"w\") as file:\n",
            "        file.write(obj_file_content) \n",
            "         "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Using only text**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            " \n",
            "from pathlib import Path\n",
            " \n",
            "folder = working_dir / 'renders'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
            " \n",
            "text_coords = [] \n",
            "for text in labels:\n",
            "    print(f\"Generating {text}\")\n",
            "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0) \n",
            "    text_coords.append(faces_coordinates)\n",
            "    \n",
            "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)  \n",
            "     \n",
            " \n",
            "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Text + prompt of tokens**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Grab fresh copy of dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = MeshDataset.load(dataset_path)\n",
            "dataset.generate_codes(autoencoder)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Prompt with 10% of codes/tokens**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "token_length_procent = 0.10 \n",
            "codes = []\n",
            "texts = []\n",
            "for label in labels:\n",
            "    for item in dataset.data: \n",
            "        if item['texts'] == label:\n",
            "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent) \n",
            "            \n",
            "            texts.append(item['texts']) \n",
            "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))  \n",
            "            break\n",
            "        \n",
            "folder = working_dir / f'renders/text+codes'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
            "\n",
            "coords = [] \n",
            "\n",
            "\n",
            "\n",
            "for text, prompt in zip(texts, codes): \n",
            "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
            "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0) \n",
            "    coords.append(faces_coordinates) \n",
            "    \n",
            "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
            "    write_mesh_output(obj_file_path, faces_coordinates)\n",
            "        \n",
            "    print(obj_file_path)\n",
            "     \n",
            " \n",
            "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
            "\n",
            "if text_coords is not None: \n",
            "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Prompt with 0% to 80% of tokens**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            " \n",
            "folder = working_dir / f'renders/text+codes_rows'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)   \n",
            "\n",
            "mesh_rows = []\n",
            "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
            "    codes = []\n",
            "    texts = []\n",
            "    for label in labels:\n",
            "        for item in dataset.data: \n",
            "            if item['texts'] == label:\n",
            "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent) \n",
            "                \n",
            "                texts.append(item['texts']) \n",
            "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))  \n",
            "                break\n",
            "            \n",
            "    coords = []   \n",
            "    for text, prompt in zip(texts, codes): \n",
            "        \n",
            "        print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
            "        faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0) \n",
            "        coords.append(faces_coordinates)\n",
            "        \n",
            "        obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
            "        write_mesh_output(obj_file_path, coords)  \n",
            "        print(obj_file_path)\n",
            "        \n",
            "        \n",
            "    mesh_rows.append(coords) \n",
            "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
            "    \n",
            "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
            " "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Just some testing for text embedding similarity**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np \n",
            "texts = list(labels)\n",
            "vectors = [transformer.conditioner.text_models[0].embed_text([text], return_text_encodings = False).cpu().flatten() for text in texts]\n",
            " \n",
            "max_label_length = max(len(text) for text in texts)\n",
            " \n",
            "# Print the table header\n",
            "print(f\"{'Text':<{max_label_length}} |\", end=\" \")\n",
            "for text in texts:\n",
            "    print(f\"{text:<{max_label_length}} |\", end=\" \")\n",
            "print()\n",
            "\n",
            "# Print the similarity matrix as a table with fixed-length columns\n",
            "for i in range(len(texts)):\n",
            "    print(f\"{texts[i]:<{max_label_length}} |\", end=\" \")\n",
            "    for j in range(len(texts)):\n",
            "        # Encode the texts and calculate cosine similarity manually\n",
            "        vector_i = vectors[i]\n",
            "        vector_j = vectors[j]\n",
            "        \n",
            "        dot_product = torch.sum(vector_i * vector_j)\n",
            "        norm_vector1 = torch.norm(vector_i)\n",
            "        norm_vector2 = torch.norm(vector_j)\n",
            "        similarity_score = dot_product / (norm_vector1 * norm_vector2)\n",
            "        \n",
            "        # Print with fixed-length columns\n",
            "        print(f\"{similarity_score.item():<{max_label_length}.4f} |\", end=\" \")\n",
            "    print()"
         ]
      }
   ],
   "metadata": {
      "kaggle": {
         "accelerator": "gpu",
         "dataSources": [],
         "dockerImageVersionId": 30627,
         "isGpuEnabled": true,
         "isInternetEnabled": true,
         "language": "python",
         "sourceType": "notebook"
      },
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
